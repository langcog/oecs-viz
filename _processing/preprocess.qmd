```{r setup}
library(dplyr)
library(glue)
library(here)
library(httr)
library(lubridate)
library(purrr)
library(stringr)
library(tibble)
library(tidyr)
library(xml2)

library(tidytext)
library(stopwords)
library(udpipe)
library(tidygraph)
library(lsa)

# slightly modified function from cbail/textnets package
# tokenizes, tags pos, counts word prevalences
source(here("_processing/helper/PrepText.R"))
```

# Article fetching and processing

```{r fetch-pubs}
# call of pubpub API for OECS
oecs_request <- \(verb, endpoint, ...) {
  url <- str_c("https:/oecs.mit.edu/api/", endpoint)
  VERB(verb, url, content_type("application/json"), accept("application/json"),
       encode = "json", ...)
}

# authenticate
pw <- Sys.getenv("OECS_PASSWORD")
creds <- list(email = "mika.br@gmail.com", password = pw)
login <- oecs_request("POST", "login", body = creds)

# get all pubs in OECS collection
collection_query <- list(collectionId = "9dd2a47d-4a84-4126-9135-50a6383c26a9",
                         communityId = "e2759450-b8e2-433a-a70d-45aff0f75d45",
                         limit = 1000)
collection_req <- oecs_request("GET", "collectionPubs", query = collection_query)
collection <- content(collection_req, "parsed") |> transpose()

pub_ids <- set_names(collection$id, collection$title)

# get each pub
get_pub <- \(pub_id, pub_title) {
  message(glue('Fetching pub "{pub_title}"'))
  pub_req <- oecs_request("GET", glue("pubs/{pub_id}"))
  pub_req |> content("parsed")
}
pubs <- imap(pub_ids, get_pub)

# get pub slugs
pub_slugs <- map(pubs, \(pub) pub |> pluck("slug"))

# get content of each pub
get_pub_content <- \(pub_id, pub_title) {
  message(glue('Fetching content for pub "{pub_title}"'))
  pub_req <- oecs_request("GET", glue("pubs/{pub_id}/text"))
  pub_req |> content("parsed")
}
pub_contents <- imap(pub_ids, possibly(get_pub_content))
```

```{r extract-text}
# transform a pub's content to only text nodes
get_pub_text_nodes <- \(pub) {
  
  pub_content <- pub |> pluck("content")
  # remove any end matter
  end_ids <- c("references", "further-reading")
  end <- pub_content |> map_lgl(\(pc) pc$type == "heading" &
                                  is.element(pc$attrs$id, end_ids))
  end_start <- if (any(end)) end |> which() |> min() else length(pub_content)
  pre_end <- pub_content[seq_len(end_start - 1)]
  
  # filter to paragraph nodes, get their content
  paragraphs <- pre_end |> keep(\(pc) pc$type == "paragraph") |>
    map(\(p) p$content) |> compact()
  
  # flatten node tree to one remaining nesting level
  flattener <- map(seq_along(pluck_depth(paragraphs) - 1), \(i) list_flatten) |>
    reduce(compose)
  nodes <- flattener(paragraphs)
  
  # filter to text nodes
  nodes |> keep(\(node) node$type == "text")
}

# get text nodes of each pub
pub_text_nodes <- map(pub_contents, get_pub_text_nodes)

# transform pub text nodes to single string
get_pub_text <- \(text_nodes) {
  
  # remove links/markup
  nodes <- text_nodes |> keep(
    \(node) !any(is.element(c("link", "em"), unlist(node$marks)))
  ) |> compact()
  
  # get node text, combine
  text <- nodes |> transpose() |> pluck("text")
  text |> paste(collapse = "") |> str_remove_all(" \\([; ]*\\)")
}

# get text string of each pub
pub_text <- map_chr(pub_text_nodes, get_pub_text)
```

```{r extract-links}
# transform pub text nodes to list of links
get_pub_crosslinks <- \(text_nodes) {

  # filter to only links
  nodes <- text_nodes |> keep(
    \(node) is.element("link", unlist(node$marks))
  ) |> compact()

  # get text and href of each link
  nodes |>
    map(\(node) {
      link <- node$marks |> keep(\(m) m$type == "link") |> flatten()
      list(href = link |> pluck("attrs", "href"),
           text = node |> pluck("text"))
    })
}

# get links of each pub
pub_crosslinks <- map(pub_text_nodes, get_pub_crosslinks)
```

```{r}
save(pub_ids, pub_slugs, pub_text, pub_crosslinks,
     file = here(glue("data/oecs_data_{today()}.RData")))

save(pub_ids, pub_slugs, pub_text, pub_crosslinks,
     file = here(glue("data/oecs_data_latest.RData")))
```

```{r compute-similarities}
# prep text for network by tokenizing / counting lemmas
pub_tibble <- pub_text |>
  enframe(name = "article", value = "text") |>
  mutate(article = str_trim(article))

udmodel <- udpipe_load_model(
  file = here("_processing/resources/english-ewt-ud-2.5-191206.udpipe"))
prepped_pubs <- PrepText(pub_tibble, groupvar = "article", textvar = "text",
                         node_type = "groups", tokenizer = "words", pos = "nouns",
                         remove_stop_words = TRUE, remove_numbers = TRUE,
                         udmodel_lang = udmodel)

# read in cached glove vectors
glove <- data.table::fread(file = here("_processing/resources/glove.6B.50d.txt"),
                           sep = " ", quote = "")

# combine pub text with vectors
pub_lemmas_glove <- prepped_pubs |>
  ungroup() |>
  inner_join(glove, by = c("lemma" = "V1"))

# take mean of vectors for each pub
pubs_glove <- pub_lemmas_glove |>
  group_by(article) |>
  summarise(across(starts_with("V"), mean))

# get pairwise similarity between pub vectors
pubs_mat <- pubs_glove |> column_to_rownames("article") |> t()
pubs_cosine <- cosine(pubs_mat)
pubs_similarity <- pubs_cosine |> as_tibble(rownames = "article1") |>
  pivot_longer(-article1, names_to = "article2", values_to = "similarity") |>
  mutate(similarity_norm = (similarity - min(similarity)) /
           (max(similarity) - min(similarity)))
```

```{r create-graph}
# graph parameters
min_edges <- 2
max_edges <- 8

pubs_similarity_sorted <- pubs_similarity |>
  filter(article1 != article2) |>
  arrange(article1, desc(similarity_norm))

# find the smallest edge weight such that no vertex has more than max_edges
# edges with similarity greater than it
min_edge_weight <- pubs_similarity_sorted |>
  group_by(article1) |>
  slice(1:max_edges) |>
  filter(similarity_norm == min(similarity_norm)) |>
  ungroup() |>
  filter(similarity_norm == max(similarity_norm)) |>
  pull(similarity_norm)

# take edges of at least min_edge_weight
pubs_similarity_threshold <- pubs_similarity_sorted |>
  filter(similarity_norm >= min_edge_weight)
# pubs_similarity_threshold |> count(article1, sort = TRUE) max n should be max_edges

# take top min_edges edges for each vertex so nothing ends up disconnected
pubs_similarity_nn <- pubs_similarity_sorted |>
  group_by(article1) |>
  slice(1:min_edges) |>
  ungroup()

# combine top edge + thresholded edges, renormalize similarities
pubs_similarity_combined <- bind_rows(pubs_similarity_threshold,
                                      pubs_similarity_nn) |>
  distinct() |>
  mutate(similarity_renorm = (similarity_norm - min(similarity_norm)) /
           (max(similarity_norm) - min(similarity_norm)))

# compute clusters and centralities
pubs_similarity_graph_combined <- pubs_similarity_combined |>
  as_tbl_graph(directed = FALSE) |>
  mutate(group = factor(group_louvain())) |>
  mutate(centrality = centrality_alpha())

# pubs_layout_combined <- igraph::layout_with_kk(pubs_similarity_graph_combined)
# ggraph(pubs_similarity_graph_combined, layout = pubs_layout_combined, weights = similarity_norm) +
#   geom_edge_link(aes(edge_alpha = similarity_norm), show.legend = FALSE) +
#   geom_node_point(aes(color = group, size = centrality), show.legend = FALSE) +
#   geom_node_text(aes(label = name), repel = TRUE, size = 3, family = .font) +
#   scale_colour_ptol(guide = "none")
```

```{r save-data}
slugs <- pub_slugs |> unlist() |> enframe(value = "slug")
save(pubs_similarity_graph_combined, slugs, file = here("data/pub_graph.RData"))
```
