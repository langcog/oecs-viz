```{r}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(glue)
library(httr)
library(purrr)
library(stringr)
library(tibble)
library(tidyr)

library(textnets)
library(tidygraph)
library(lsa)

.font <- "Noto Sans"
sysfonts::font_add_google(.font, .font)
theme_set(theme_bw(base_size = 14, base_family = .font))
theme_update(panel.grid = element_blank(),
             strip.background = element_blank(),
             legend.key = element_blank(),
             panel.border = element_blank(),
             axis.line = element_line(),
             strip.text = element_text(face = "bold"))
```

# Article fetching and processing

```{r}
# call of pubpub API for OECS
oecs_request <- \(verb, endpoint, ...) {
  url <- str_c("https:/oecs.mit.edu/api/", endpoint)
  VERB(verb, url, content_type("application/json"), accept("application/json"),
       encode = "json", ...)
}

# authenticate
pw <- Sys.getenv("OECS_PASSWORD")
creds <- list(email = "mika.br@gmail.com", password = pw)
login <- oecs_request("POST", "login", body = creds)
# content(login, "text")

# get all pubs in OECS collection
collection_query <- list(collectionId = "9dd2a47d-4a84-4126-9135-50a6383c26a9",
                         communityId = "e2759450-b8e2-433a-a70d-45aff0f75d45",
                         limit = 200)
collection_req <- oecs_request("GET", "collectionPubs", query = collection_query)
collection <- content(collection_req, "parsed") |> transpose()

pub_ids <- set_names(collection$id, collection$title)

# get each pub
get_pub <- \(pub_id, pub_title) {
  message(glue('Fetching pub "{pub_title}"'))
  pub_req <- oecs_request("GET", glue("pubs/{pub_id}"))
  pub_req |> content("parsed")
}
pubs <- imap(pub_ids, get_pub)

# get pub slugs
pub_slugs <- map(pubs, \(pub) pub |> pluck("slug"))

# get content of each pub
get_pub_content <- \(pub_id, pub_title) {
  message(glue('Fetching content for pub "{pub_title}"'))
  pub_req <- oecs_request("GET", glue("pubs/{pub_id}/text"))
  pub_req |> content("parsed")
}
pub_contents <- imap(pub_ids, get_pub_content)

# transform a pub's content to only text nodes
get_pub_text_nodes <- \(pub) {
  
  pub_content <- pub |> pluck("content")
  # remove any end matter
  end_ids <- c("references", "further-reading")
  end <- pub_content |> map_lgl(\(pc) pc$type == "heading" &
                                  is.element(pc$attrs$id, end_ids))
  end_start <- if (any(end)) end |> which() |> min() else length(pub_content)
  pre_end <- pub_content[seq_len(end_start - 1)]
  
  # filter to paragraph nodes, get their content
  paragraphs <- pre_end |> keep(\(pc) pc$type == "paragraph") |>
    map(\(p) p$content) |> compact()
  
  # flatten node tree to one remaining nesting level
  flattener <- map(seq_along(pluck_depth(paragraphs) - 1), \(i) list_flatten) |>
    reduce(compose)
  nodes <- flattener(paragraphs)
  
  # filter to text nodes
  nodes |> keep(\(node) node$type == "text")
}

# get text nodes of each pub
pub_text_nodes <- map(pub_contents, get_pub_text_nodes)

# transform pub text nodes to single string
get_pub_text <- \(text_nodes) {
  
  # remove links/markup
  nodes <- text_nodes |> keep(
    \(node) !any(is.element(c("link", "em"), unlist(node$marks)))
  ) |> compact()
  
  # get node text, combine
  text <- nodes |> transpose() |> pluck("text")
  text |> paste(collapse = "") |> str_remove_all(" \\([; ]*\\)")
}

# get text string of each pub
pub_text <- map_chr(pub_text_nodes, get_pub_text)

# # transform pub text nodes to list of links
# get_pub_crosslinks <- \(text_nodes) {
#   
#   # filter to only links
#   nodes <- text_nodes |> keep(
#     \(node) is.element("link", unlist(node$marks))
#   ) |> compact()
#   
#   # get text and href of each link
#   nodes |>
#     map(\(node) {
#       link <- node$marks |> keep(\(m) m$type == "link") |> flatten()
#       list(href = link |> pluck("attrs", "href"),
#            text = node |> pluck("text"))
#     })
# }
# 
# # get links of each pub
# pub_crosslinks <- map(pub_text_nodes, get_pub_crosslinks)
```

```{r}
# # identify incorrect crosslink hrefs
# check_crosslink <- \(crosslink) {
#   href <- if (is.null(crosslink$href)) "" else crosslink$href
#   article <- crosslink$text
#   
#   # ignore external/section hrefs
#   if (href != "" &&
#       (str_detect(href, "#.*$") | !str_detect(href, "oecs"))) return(NULL)
#   
#   # check article exists and its slug is right
#   slug <- href |> str_extract("[A-z0-9]+$")
#   target_slug <- pub_slugs[[article]]
#   good_slug <- !is.na(slug) && !is.null(target_slug) && slug == target_slug
#   # good_slug <- article %in% names(pub_slugs) &&
#   #   !is.na(slug) && slug == pub_slugs[[article]]
#   corrected <- if (is.null(target_slug)) NULL else paste0("https://oecs.mit.edu/pub/", target_slug)
#   if (!good_slug) return(tibble(to = article, href = href,
#                                 href_corrected = corrected))
# }
# 
# pub_crosslink_checks <- pub_crosslinks |> compact() |>
#   map(\(crosslinks) crosslinks |> map_df(check_crosslink))
# bad_crosslinks <- pub_crosslink_checks |> compact() |>
#   bind_rows(.id = "from") #|> arrange(from)
# 
# # identify incorrect "see $article" statements
# check_see <- \(text) {
#   sees <- text |> str_extract_all("\\[see [A-z]*?\\]") |> unlist() |>
#     str_extract("\\[see ([A-z]*?)\\]", group = TRUE) |> keep(\(s) str_length(s) > 0)
#   sees[!(sees %in% names(pub_ids))]
# }
# 
# pub_miss <- map(pub_text, check_see) |> compact()
```

```{r}
# prep text for network by tokenizing / counting lemmas
pub_tibble <- pub_text |>
  enframe(name = "article", value = "text") |>
  mutate(article = str_trim(article))
prepped_pubs <- PrepText(pub_tibble, groupvar = "article", textvar = "text",
                         node_type = "groups", tokenizer = "words", pos = "nouns",
                         remove_stop_words = TRUE, remove_numbers = TRUE)

# read in cached glove vectors
glove <- data.table::fread(file = "resources/glove.6B.50d.txt", sep = " ", quote = "")

# combine pub text with vectors
pub_lemmas_glove <- prepped_pubs |>
  ungroup() |>
  inner_join(glove, by = c("lemma" = "V1"))

# take mean of vectors for each pub
pubs_glove <- pub_lemmas_glove |>
  group_by(article) |>
  summarise(across(starts_with("V"), mean))

# get pairwise similarity between pub vectors
pubs_mat <- pubs_glove |> column_to_rownames("article") |> t()
pubs_cosine <- cosine(pubs_mat)
pubs_similarity <- pubs_cosine |> as_tibble(rownames = "article1") |>
  pivot_longer(-article1, names_to = "article2", values_to = "similarity") |>
  mutate(similarity_norm = (similarity - min(similarity)) /
           (max(similarity) - min(similarity)))
```

```{r}
pubs_similarity_sorted <- pubs_similarity |>
  filter(article1 != article2) |>
  arrange(article1, desc(similarity_norm))

# find the smallest edge weight such that no vertex has more than max_edges
# edges with similarity greater than it
# min_edge_weight <- 0.85
max_edges <- 8
min_edge_weight <- pubs_similarity_sorted |>
  group_by(article1) |>
  slice(1:max_edges) |>
  filter(similarity_norm == min(similarity_norm)) |>
  ungroup() |>
  filter(similarity_norm == max(similarity_norm)) |>
  pull(similarity_norm)

# take edges of at least min_edge_weight
pubs_similarity_threshold <- pubs_similarity_sorted |>
  filter(similarity_norm >= min_edge_weight)
# pubs_similarity_threshold |> count(article1, sort = TRUE) max n should be max_edges

# take top min_edges edges for each vertex so nothing ends up disconnected
min_edges <- 2
pubs_similarity_nn <- pubs_similarity_sorted |>
  group_by(article1) |>
  slice(1:min_edges) |>
  ungroup()

# combine top edge + thresholded edges, renormalize similarities
pubs_similarity_combined <- bind_rows(pubs_similarity_threshold,
                                      pubs_similarity_nn) |>
  distinct() |>
  mutate(similarity_renorm = (similarity_norm - min(similarity_norm)) /
           (max(similarity_norm) - min(similarity_norm)))

# compute clusters and centralities
pubs_similarity_graph_combined <- pubs_similarity_combined |>
  as_tbl_graph(directed = FALSE) |>
  mutate(group = factor(group_louvain())) |>
  mutate(centrality = centrality_alpha())

# pubs_layout_combined <- igraph::layout_with_kk(pubs_similarity_graph_combined)
# ggraph(pubs_similarity_graph_combined, layout = pubs_layout_combined, weights = similarity_norm) +
#   geom_edge_link(aes(edge_alpha = similarity_norm), show.legend = FALSE) +
#   geom_node_point(aes(color = group, size = centrality), show.legend = FALSE) +
#   geom_node_text(aes(label = name), repel = TRUE, size = 3, family = .font) +
#   scale_colour_ptol(guide = "none")
```

```{r}
slugs <- pub_slugs |> unlist() |> enframe(value = "slug")
save(pubs_similarity_graph_combined, slugs, file = "pub_data.RData")
```
